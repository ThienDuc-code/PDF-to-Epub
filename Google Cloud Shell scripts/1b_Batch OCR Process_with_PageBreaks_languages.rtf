{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 #!/usr/bin/env bash\
# 1_Batch OCR Process (with page breaks + language hints)\
\
set -euo pipefail\
\
# ===== config =====\
PROJECT="silver-spark-473016-t8"\
LOCATION="eu"\
# Your existing processor version (pretrained OCR)\
PROCESSOR_VERSION="projects/99021705772/locations/eu/processors/ddb44ccc2e0a7601/processorVersions/pretrained-ocr-v2.0-2023-06-02"\
\
IN_PREFIX="gs://pdf-ocr-books/HOL_Vol_1_to_12"\
DATE_PREFIX="$(date +%Y-%m-%d)"\
OUT_PREFIX="gs://pdf-ocr-books/docai-output/$\{DATE_PREFIX\}/batch_clean"\
\
ACCESS_TOKEN="$(gcloud auth print-access-token)"\
\
# ===== collect PDFs =====\
mapfile -t PDFS < <(gsutil ls "$\{IN_PREFIX\}/*.pdf")\
if [ $\{#PDFS[@]\} -eq 0 ]; then\
  echo "No PDFs at $\{IN_PREFIX\}"\
  exit 1\
fi\
\
# ===== build JSON payload for batch =====\
# Adds OCR language hints for en, fr, it, la\
REQ_FILE="$(mktemp)"\
\{\
  printf '\{\\n'\
  printf '  "inputDocuments": \{ "gcsDocuments": \{ "documents": [\\n'\
  first=1\
  for f in "$\{PDFS[@]\}"; do\
    [ $first -eq 0 ] && printf ',\\n'\
    printf '    \{ "gcsUri": "%s", "mimeType": "application/pdf" \}' "$f"\
    first=0\
  done\
  printf '\\n  ] \} \},\\n'\
  printf '  "processOptions": \{\\n'\
  printf '    "ocrConfig": \{\\n'\
  printf '      "languageHints": ["en","fr","it","la"]\\n'\
  printf '    \}\\n'\
  printf '  \},\\n'\
  printf '  "documentOutputConfig": \{ "gcsOutputConfig": \{ "gcsUri": "%s" \} \}\\n' "$OUT_PREFIX"\
  printf '\}\\n'\
\} > "$REQ_FILE"\
\
# ===== kick off batch =====\
OP_JSON="$(mktemp)"\
curl -sS -X POST \\\
  -H "Authorization: Bearer $\{ACCESS_TOKEN\}" \\\
  -H "Content-Type: application/json; charset=utf-8" \\\
  -d @"$REQ_FILE" \\\
  "https://$\{LOCATION\}-documentai.googleapis.com/v1/$\{PROCESSOR_VERSION\}:batchProcess" \\\
  > "$OP_JSON"\
\
OP_NAME="$(jq -r '.name // empty' "$OP_JSON")"\
if [ -z "$OP_NAME" ]; then\
  echo "Failed to start batch:"\
  cat "$OP_JSON"\
  exit 1\
fi\
\
echo "Batch started: $OP_NAME"\
\
# ===== poll until done =====\
while true; do\
  sleep 5\
  curl -sS -H "Authorization: Bearer $\{ACCESS_TOKEN\}" \\\
    "https://$\{LOCATION\}-documentai.googleapis.com/v1/$\{OP_NAME\}" > "$OP_JSON"\
  DONE="$(jq -r '.done // false' "$OP_JSON")"\
  [ "$DONE" = "true" ] && break\
  echo -n "."\
done\
echo; echo "Batch finished."\
\
# ===== merge per-volume JSON \uc0\u8594  TXT with page breaks =====\
# We\'92ll extract text per page using the page text anchors, and join with ---Page-Break---\
SCRATCH="$HOME/docai_text_$\{DATE_PREFIX\}"\
mkdir -p "$SCRATCH"\
\
# Ensure jq is available (Cloud Shell usually has it)\
if ! command -v jq >/dev/null 2>&1; then\
  sudo apt-get update -y && sudo apt-get install -y jq\
fi\
\
# For each volume output directory under OUT_PREFIX/*/\
while read -r VOL_DIR; do\
  # VOL_DIR looks like: gs://.../batch_clean/<VOL>/<operation-id>/0/\
  # We\'92ll place the merged txt next to the <VOL>/ level (same as your original)\
  STEM="$(basename "$VOL_DIR" | sed 's#/##')"\
  PARENT_DIR="$(dirname "$VOL_DIR")"             # .../<VOL>/<operation-id>/\
  VOL_PARENT="$(dirname "$PARENT_DIR")/"         # .../<VOL>/\
  DEST_TXT="$\{VOL_PARENT\}$\{STEM\}_ocr.txt"        # keep your naming convention\
\
  echo "==> Merging text (with page breaks) for $\{STEM\}"\
  LOCAL_DIR="$\{SCRATCH\}/$\{STEM\}"\
  rm -rf "$LOCAL_DIR" && mkdir -p "$LOCAL_DIR"\
\
  # Pull all JSON shards for this volume\
  if ! gsutil -m cp "$\{VOL_DIR\}"**.json "$LOCAL_DIR/" 2>/dev/null; then\
    echo "   (no JSONs found in $\{VOL_DIR\})"\
    continue\
  fi\
\
  # Sort all JSONs naturally and extract page-wise text:\
  # - Each JSON has a top-level "text" and a "pages" array.\
  # - Each page has layout.textAnchor.textSegments with start/end indexes into .text.\
  # We slice .text for each page and join with the page-break marker.\
  MERGED="$SCRATCH/$\{STEM\}_ocr.txt"\
  printf '%s\\0' "$LOCAL_DIR"/*.json \\\
    | sort -z -V \\\
    | xargs -0 -I\{\} jq -r '\
      # For each shard, emit one big string that contains all pages separated by the marker.\
      def page_texts:\
        if (.pages and .text) then\
          [ .pages[]\
            | ( .layout.textAnchor.textSegments // [] )\
            | map( . as $seg | (.startIndex // 0) as $s | (.endIndex // 0) as $e | (.text[$s:$e]) )\
            | join("")\
          ]\
        else\
          [] # Fallback: no pages or no text\
        end;\
\
      # Prefer true per-page extraction; if not available, fall back to whole .text\
      ( if ((.pages|type) == "array") and (.text|type) == "string" and (.pages|length) > 0\
        then (page_texts | join("\\n---Page-Break---\\n") + "\\n---Page-Break---\\n")\
        elif (.text|type) == "string" then (.text + "\\n---Page-Break---\\n")\
        else "" end\
      )\
    ' "\{\}" \\\
    > "$MERGED"\
\
  # Basic tidy: drop leading blank lines; keep rest as-is\
  awk 'NF\{p=1\}p' "$MERGED" > "$\{MERGED%.txt\}_clean.txt"\
\
  # Upload next to the volume folder root (like your original)\
  gsutil cp "$\{MERGED%.txt\}_clean.txt" "$DEST_TXT"\
  echo "   -> $DEST_TXT"\
done < <(gsutil ls -d "$\{OUT_PREFIX\}"/*/ 2>/dev/null)\
\
echo "All done. Output under: $\{OUT_PREFIX\}"\
}